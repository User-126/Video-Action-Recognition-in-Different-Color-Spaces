{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.models.video import r3d_18\nimport numpy as np\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Custom dataset class for loading videos using OpenCV\nclass VideoDataset(Dataset):\n    def __init__(self, video_files, labels, transform=None, num_frames=16):\n        self.video_files = video_files\n        self.labels = labels\n        self.transform = transform\n        self.num_frames = num_frames  # Number of frames to sample from each video\n\n    def __len__(self):\n        return len(self.video_files)\n\n    def __getitem__(self, idx):\n        video_path = self.video_files[idx]\n        label = self.labels[idx]\n\n        # Read video using OpenCV\n        cap = cv2.VideoCapture(video_path)\n        frames = []\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_indices = np.linspace(0, frame_count - 1, self.num_frames, dtype=int)\n\n        for i in range(frame_count):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            # Only collect frames at the specified indices\n            if i in frame_indices:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2Lab)  # Convert BGR to RGB\n                if self.transform:\n                    frame = self.transform(frame)\n                frames.append(frame)\n\n        cap.release()\n\n        # Ensure we have the correct number of frames\n        if len(frames) != self.num_frames:\n            # If not enough frames, pad with zeros\n            padding = self.num_frames - len(frames)\n            frames += [torch.zeros_like(frames[0])] * padding\n\n        video_tensor = torch.stack(frames).permute(1, 0, 2, 3)  # Rearrange to CxTxHxW\n        return video_tensor, label\n\n# Prepare dataset\ndata_dir = '/content/drive/MyDrive/Sign Language Dataset'\nclasses = sorted(os.listdir(data_dir))\nvideo_files = []\nlabels = []\n\nfor class_idx, class_name in enumerate(classes):\n    class_folder = os.path.join(data_dir, class_name)\n    for video in os.listdir(class_folder):\n        video_path = os.path.join(class_folder, video)\n        video_files.append(video_path)\n        labels.append(class_idx)\n\n# Split the dataset into 80% training and 20% testing\ntrain_videos, test_videos, train_labels, test_labels = train_test_split(video_files, labels, test_size=0.2, stratify=labels, random_state=42)\n\n# Data transformations (resize, crop, tensorize, normalize)\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((128, 128)),\n    transforms.CenterCrop(112),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n])\n\n# Create datasets\ntrain_dataset = VideoDataset(train_videos, train_labels, transform=transform)\ntest_dataset = VideoDataset(test_videos, test_labels, transform=transform)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models.video import r3d_18\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Load pre-trained 3D ResNet model\nmodel = r3d_18(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 16)  # Modify output layer for 16 classes\nmodel = model.to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training and Validation function with accuracy/loss tracking\ndef train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=10):\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        # Training Loop with tqdm\n        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        train_accuracy = 100 * correct_train / total_train\n        train_losses.append(train_loss)\n        train_accuracies.append(train_accuracy)\n\n        # Validation Step\n        model.eval()\n        correct_val = 0\n        total_val = 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in tqdm(test_loader, desc='Validating', leave=False):\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n\n        val_loss = val_loss / len(test_loader)\n        val_accuracy = 100 * correct_val / total_val\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n\n        # Print statistics\n        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n\n    return train_losses, train_accuracies, val_losses, val_accuracies\n\n# Function to plot the accuracy and loss curves\ndef plot_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n    epochs = range(1, len(train_losses) + 1)\n\n    # Plot Losses\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, 'r', label='Training loss')\n    plt.plot(epochs, val_losses, 'b', label='Validation loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Plot Accuracies\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, 'r', label='Training accuracy')\n    plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.show()\n\n# Function to evaluate the model and print the classification report and confusion matrix\ndef evaluate_model(model, test_loader, device, class_names):\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n\n    # Classification report\n    report = classification_report(y_true, y_pred, target_names=class_names)\n    print(\"Classification Report:\\n\", report)\n\n    # Confusion Matrix\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.show()\n\n    return y_true, y_pred\n\n# t-SNE Visualization\ndef tsne_visualization(model, test_loader, device, num_samples=500):\n    model.eval()\n    features = []\n    labels = []\n    with torch.no_grad():\n        for i, (inputs, targets) in enumerate(test_loader):\n            if len(labels) > num_samples:\n                break\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            features.append(outputs.cpu().numpy())\n            labels.extend(targets.numpy())\n\n    features = np.vstack(features)\n    tsne = TSNE(n_components=2, random_state=42)\n    features_tsne = tsne.fit_transform(features)\n\n    plt.figure(figsize=(10, 8))\n    sns.scatterplot(x=features_tsne[:, 0], y=features_tsne[:, 1], hue=labels, palette='deep', s=60)\n    plt.title('t-SNE of Video Features')\n    plt.show()\n\n\n\n# Train the model with validation\ntrain_losses, train_accuracies, val_losses, val_accuracies = train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=10)\n\n# Plot accuracy and loss curves\nplot_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n\n# Evaluate the model and get classification report and confusion matrix\nclass_names = sorted(os.listdir(data_dir))  # Assuming class names are folder names\ny_true, y_pred = evaluate_model(model, test_loader, device, class_names)\n\n\n# t-SNE visualization of features\ntsne_visualization(model, train_loader, device)\n\n","metadata":{},"outputs":[],"execution_count":null}]}
